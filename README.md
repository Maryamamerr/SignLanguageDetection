# SignLanguageDetection
ğŸ§  Sign Language Recognition using LSTM and MediaPipe
This project is a real-time sign language recognition system using MediaPipe, OpenCV, and LSTM neural networks. It can detect and classify hand gestures like "hello", "thanks", and "I love you" using a webcam, and outputs the prediction using on-screen display and text-to-speech.
# ğŸ“ Project Structure
 ```
â”œâ”€â”€ main.py               # Run this to start real-time sign detection
â”œâ”€â”€ preproccessing.py     # Data collection and preprocessing
â”œâ”€â”€ training.py           # Train LSTM model on extracted keypoints
â”œâ”€â”€ action.h5             # Saved trained model (generated after training)
â”œâ”€â”€ MP_Data/              # Folder where keypoint data is saved
â””â”€â”€ README.md             # Project documentation
 ```
# ğŸ”§ Dependencies
Install required packages:
 ```
pip install opencv-python mediapipe tensorflow scikit-learn pyttsx3
 ```
# ğŸ§© How It Works
1. Data Collection (preproccessing.py)
Captures MediaPipe keypoints (face, pose, hands) from webcam.
Saves them as .npy files for each gesture (hello, thanks, i love you).

Run:
```
python preproccessing.py
```
3. Model Training (training.py)
Loads saved keypoints
Trains a 3-layer LSTM model
Saves the model as action.h5

Run:
```
python training.py
```
3. Real-time Prediction (main.py)
Loads the trained model
Captures webcam input
Predicts the performed gesture
Displays probabilities on screen
Speaks the prediction using pyttsx3

Run:
```
python main.py
```

ğŸ“Š Model Performance
* Accuracy is printed after training
* Confusion matrix helps visualize prediction quality
  
âœ¨ Features
* Real-time hand gesture detection
* Works offline (no internet required)
* Text-to-speech output
* Modular code for easy expansion

  ğŸ¤ Credits
* Developed using:

* MediaPipe

* TensorFlow

* OpenCV
